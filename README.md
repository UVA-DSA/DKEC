## Presentation
One presentation ppt is available from [here](https://docs.google.com/presentation/d/1UDghDmYYrFjqUCDl9Q_15gfOCsv00Yur/edit#slide=id.p1)

## Dataset
* EMS dataset
  - EMS dataset is not publicly available, however I uploaded the processed dataset into folder 
*/data/multi_label*
- MIMIC3 dataset
  - MIMIC3 dataset is publicly available. Refer to [this page](https://physionet.org/content/mimiciii/1.4/) to apply online.


## Environment
I am training and testing on both local machines and clusters. The environment is specified as follows,
### Local Machine
Ubuntu 22.04, NVIDIA GeForce RTX 3090 (24G), CUDA 11.4, Driver version 470.161.03. Check requirements.txt
to install all required packages. run with `python main.py`.
### Rivanna cluster
* Creating docker images:
Rivanna has their own "pre-built" docker images in their [git repo](https://github.com/uvarc/rivanna-docker)
you can download one of them according to [link](https://www.rc.virginia.edu/userinfo/howtos/rivanna/docker-images-on-rivanna/). 
I used pytorch 1.12.0 which already had cuda and pytorch installed.

* Install all dependencies: 
The basic command is **singularity exec <container.sif> python -m pip install --user <package>**
Check the [link](https://www.rc.virginia.edu/userinfo/howtos/rivanna/add-packages-to-container/) for more details.

* A pre-built docker image is available in docker hub [repo](https://hub.docker.com/repository/docker/masqueraderx/emnlp_2023/general),
it has installed CUDA, pytorch and all dependencies for this work.

* Run with bash file:
See an example in **run.slurm**, more details can be seen from this [link](https://www.rc.virginia.edu/userinfo/rivanna/slurm/)
Run with `sbatch run.slurm*`

* request a node
ijob -A uva-dsa -w udc-an34-1 -p gpu --gres=gpu -c 8 -t 01:00:00
* torchrun 

## Note

### Generate train / val / test:
- RAA dataset: train, val, test dataset can be generated by running notebook
`GT-Label Extraction.ipynb`, in order to run it, you will need to use raw RAA data
**Complete RAA Data with Protocols.xlsx**, and 'hier2p.json', 'p2hier.json'. Here, I've generated
it, you can find files in data/multi_label
- MIMIC-III dataset: run notebook `dataproc_mimic_III.ipynb`, you need to download pre-trained embeddings
`BioWordVec_PubMed_MIMICIII_d200.vec.bin` from [link](https://github.com/ncbi-nlp/BioSentVec).

### Default_sets.py
There are also some settings in this file needs to be modified when running specific experiments.
Some common variables need to be changed is **date**, **task**, **p_node**, **ungrouped_p_node**.
* date: specify the name of experiment folders
* task: specify the task is 'multi_label' or 'multi_class'
* dataset: specify which dataset is being used 'EMS' or 'MIMIC3'
* groupby: specify groupby as 'age' or 'hierarchy' or 'None'


### Config.yaml
Configure config.yaml, common configuration might be 'loss_func', 'backbone', 'attn',
'graph', 'cluster'. 

* When doing multi-label/multi-class classification, change 'loss_func' as 'BCE'/'CE'.
* Group Experiments: change 'cluster' as 'group' or 'ungroup'.
* Label-wise attention: configure 'attn' as 'la', 'graph' as 'hetero'.
* Some common configurations (more details can be seen from next section)
  * COReBERT + ungroup ('ungroup')
  * COReBERT + ungroup + label-wise-attention ('la', 'hetero)
  * COReBERT + group ('group')
  * COReBERT + group + label-wise attention ('la', 'hetero')

  
## Comparison Experiments Configuration
In the following experiments, best learning_rate and batch_size usually depend on what model you
are using. In my experience, CNN(1e-3, 16), BioBERT/COReBERT(1e-4, 16), BioGPT/GatorTron (1e-5, 8), BioMedLM(1e-5, 4)


* Pure Transformer (BioBERT vs COReBERT vs GatorTron)

| Parameters | learning_rate | batch_size | epochs | weight_decay | max_len | backbone    | attn  | cluster  | fusion | cls  |
|------------|---------------|------------|--------|--------------|---------|-------------|-------|----------|--------|------|
| Value      | 1e-4 / 1e-5   | 16 / 8     | 15     | 1e-5         | 512     | **options** | None  | ungroup  | None   | fc   |


* Label-wise attention vs Multi-head attention

| Parameters | learning_rate | batch_size | epochs | weight_decay | max_len | backbone    | attn        | cluster  | fusion | cls  |
|------------|---------------|------------|--------|--------------|---------|-------------|-------------|----------|--------|------|
| Value      | 1e-4 / 1e-5   | 16 / 8     | 15     | 1e-5         | 512     | **options** | la / qkv-la | ungroup  | None   | fc   |


* Group by age

| Parameters | learning_rate | batch_size | epochs | weight_decay | max_len | backbone    | attn | cluster | fusion | cls  |
|------------|---------------|------------|--------|--------------|---------|-------------|------|---------|--------|------|
| Value      | 1e-4 / 1e-5   | 16 / 8     | 15     | 1e-5         | 512     | **options** | None | group   | None   | fc   |


* Combine all

| Parameters | learning_rate | batch_size | epochs | weight_decay | max_len | backbone    | attn        | cluster | fusion | cls  |
|------------|---------------|------------|--------|--------------|---------|-------------|-------------|---------|--------|------|
| Value      | 1e-4 / 1e-5   | 16 / 8     | 15     | 1e-5         | 512     | **options** | la / qkv-la | group   | None   | fc   |



* ZAGCNN

| Parameters | learning_rate | batch_size | epochs | weight_decay | max_len | backbone    | attn        | cluster | fusion      | cls  |
|------------|---------------|------------|--------|--------------|---------|-------------|-------------|---------|-------------|------|
| Value      | 1e-4          | 16         | 15     | 1e-5         | 512     | **options** | la / qkv-la | ungroup | concatenate | fc   |


* KAMG
  
Also need to specify the parameter **"multi_graph=True"** in default_sets.

| Parameters | learning_rate | batch_size | epochs | weight_decay | max_len | backbone    | attn        | cluster | fusion      | cls  |
|------------|---------------|------------|--------|--------------|---------|-------------|-------------|---------|-------------|------|
| Value      | 1e-4          | 16         | 15     | 1e-5         | 512     | **options** | la / qkv-la | ungroup | concatenate | fc   |
